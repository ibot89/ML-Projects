{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation on the HR_Churn Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This project uses a dataset from Kaggle and attemps to predict whether or not a certain employee will leave the company\n",
    "or not. \n",
    "The data set analysis is performed in HR_Churn Project and will not be provided here.\n",
    "The Project uses Decision Trees as an algorithm to predict whether or not a certain employee will leave the company.\n",
    "The project starts by using sklearn to make the prediction. \n",
    "It then provides an imlementation of Decision Trees from first principles i.e without Decision Trees to attemp to re-create the same prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X**  - Imput Matrix of Size NxD where N is the number of samples and D is the number of features\n",
    "\n",
    "**Y** - Represents the targets. For binary classification, the outputs will be either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from datetime import datetime \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Botev/Desktop/logreg/HR_comma_sep.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Normalising the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"sat_level_norm\"] = (df[\"satisfaction_level\"] - df[\"satisfaction_level\"].mean())/(df[\"satisfaction_level\"].max()-df[\"satisfaction_level\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"last_evaluation_norm\"] = (df[\"last_evaluation\"] - df[\"last_evaluation\"].mean())/(df[\"last_evaluation\"].max()-df[\"last_evaluation\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"average_montly_hours_norm\"] = (df[\"average_montly_hours\"] - df[\"average_montly_hours\"].mean())/(df[\"average_montly_hours\"].max()-df[\"average_montly_hours\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"number_project_norm\"] = (df[\"number_project\"] - df[\"number_project\"].mean())/(df[\"number_project\"].max()-df[\"number_project\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding the salary Column\n",
    "def salary_encode(salary):\n",
    "    if salary ==\"low\":\n",
    "        return 1\n",
    "    elif salary==\"medium\":\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"salary_encoded\"] = df[\"salary\"].apply(salary_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[[\"satisfaction_level\",\"last_evaluation\",\"number_project\",\"average_montly_hours\",\"time_spend_company\",\"Work_accident\",\"promotion_last_5years\"]]\n",
    "y = df[\"left\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spliting the data into training and testing. The training and testing data sizes are equal.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98      5708\n",
      "          1       0.93      0.96      0.95      1792\n",
      "\n",
      "avg / total       0.97      0.97      0.97      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5576  132]\n",
      " [  63 1729]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:97%\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy:%d\"%(int(fit.score(X_test,y_test)*100))+'%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:99%\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy:%d\"%(int(fit.score(X_train,y_train)*100))+'%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> The Sklearn Decision Tree implementation yields a training accuracy of 99% and testing accuracy of 97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HR_Churn Project - Decision Tree Implementation Without Using Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This part contains an implementation for Decision Trees Algorithm without Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Maipulating functions for HR_Churn Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(X,Y):\n",
    "    \"\"\"\n",
    "    Performs one hot encoding for the salary column of dataset\n",
    "    \"\"\"\n",
    "    X_mat = X.as_matrix()#conver to matrixes \n",
    "    Y_mat = Y.as_matrix()\n",
    "    N,D = X_mat.shape\n",
    "    X2 = np.zeros((N,D+3))# introducing a second matrix. \n",
    "    #This matrix has 3 more column than the original so that the encoding \n",
    "    #can be performed\n",
    "    X2[:,0:D-1] = X_mat[:,0:D-1]\n",
    "    \n",
    "    \n",
    "    for n in xrange(N):\n",
    "        i=int(X_mat[n,D-1])\n",
    "        X2[n,D-1+i]=1\n",
    "    \n",
    "    return X2,Y_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def salary_encode(salary):\n",
    "    \"\"\"\n",
    "    Converts the categorical variables for the salary into numerical\n",
    "    \"\"\"\n",
    "    if salary ==\"low\":\n",
    "        return 1\n",
    "    elif salary==\"medium\":\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_hr_churn():\n",
    "    \"\"\"\n",
    "    Reads the data first, then normalises certain columns of the dataset\n",
    "    Applies salary_encode to get rid of the categorical variables,then applies\n",
    "    encode to perform one hot encoding. Returns the X and Y as numpy arrays in\n",
    "    2D form, similar to a matrix.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"C:/Users/Botev/Desktop/logreg/HR_comma_sep.csv\")\n",
    "    #first column normalisation\n",
    "    df[\"sat_level_norm\"] = (df[\"satisfaction_level\"] - \n",
    "    df[\"satisfaction_level\"].mean())/(df[\"satisfaction_level\"].max()-\n",
    "                                              df[\"satisfaction_level\"].min())\n",
    "    #second column normalisation\n",
    "    df[\"average_montly_hours_norm\"] = (df[\"average_montly_hours\"] - \n",
    "    df[\"average_montly_hours\"].mean())/(df[\"average_montly_hours\"].max()-\n",
    "                                              df[\"average_montly_hours\"].min())\n",
    "    #third column normalisation\n",
    "    df[\"time_spend_company_norm\"] = (df[\"time_spend_company\"] - \n",
    "    df[\"time_spend_company\"].mean())/(df[\"time_spend_company\"].max()-\n",
    "                                      df[\"time_spend_company\"].min())\n",
    "    #forth column normalisation\n",
    "    \n",
    "    df[\"number_project_norm\"] = (df[\"number_project\"] - \n",
    "    df[\"number_project\"].mean())/(df[\"number_project\"].max()-\n",
    "                                  df[\"number_project\"].min())\n",
    "    \n",
    "    \n",
    "    df[\"salary_encoded\"] = df[\"salary\"].apply(salary_encode)\n",
    "    #encodes the salary creating \n",
    "    \n",
    "    \n",
    "    X_new= df[[\"satisfaction_level\",\"last_evaluation\",\"Work_accident\",\n",
    "    \"promotion_last_5years\",\"number_project_norm\",\"time_spend_company_norm\",\n",
    "    \"average_montly_hours_norm\",\"salary_encoded\"]]\n",
    "    Y_new = df[\"left\"]\n",
    "    \n",
    "    Xout,Yout = encode(X_new,Y_new)\n",
    "    \n",
    "    return Xout,Yout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Calculates entropy\n",
    "    \"\"\"\n",
    "    # assume y is binary - 0 or 1\n",
    "    N = len(y)\n",
    "    s1 = (y == 1).sum()\n",
    "    if 0 == s1 or N == s1:\n",
    "        return 0\n",
    "    p1 = float(s1) / N\n",
    "    p0 = 1 - p1\n",
    "    return -p0*np.log2(p0) - p1*np.log2(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, depth=0, max_depth=None):\n",
    "        # print 'depth:', depth\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Checks for bases cases and information gain eqaul to 0.\n",
    "        Finds the attribute where the split occurs(self.col) as well as \n",
    "        the value at which the spliting occurs(self.split). \n",
    "        Checks if the maximum depth has been reached. If yes, aggregate all \n",
    "        values to the left and to the right. If not continue splitting. \n",
    "        Every time a split has occured the depth needs to be incremented by\n",
    "        one. \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        In this case we are checking for two main cases:\n",
    "        Case1 -  When we have a single sample\n",
    "        Case2 -  When we have a single class\n",
    "        In both of the cases no  predictions will be made\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(Y) == 1 or len(set(Y)) == 1:\n",
    "        \n",
    "            #chacks for two base cases: only 1 sameple and only one class\n",
    "            #We do not make predictions here \n",
    "            self.col = None\n",
    "            self.split = None\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.prediction = Y[0]\n",
    "\n",
    "        else:\n",
    "            D = X.shape[1]\n",
    "            cols = range(D)\n",
    "\n",
    "            max_info_gain = 0\n",
    "            best_col = None\n",
    "            best_split = None\n",
    "            for col in cols:\n",
    "                info_gain, split = self.find_split(X, Y, col)\n",
    "                if info_gain > max_info_gain:\n",
    "                    max_info_gain = info_gain\n",
    "                    best_col = col\n",
    "                    best_split = split\n",
    "\n",
    "            if max_info_gain == 0:\n",
    "                # no further splits will be performed\n",
    "                self.col = None\n",
    "                self.split = None\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "                self.prediction = np.round(Y.mean())\n",
    "            else:\n",
    "                self.col = best_col#this is the attribute we split on\n",
    "                self.split = best_split#this is the value from that attribute\n",
    "                #we have split on\n",
    "\n",
    "                if self.depth == self.max_depth:\n",
    "                    self.left = None\n",
    "                    self.right = None\n",
    "                    self.prediction = [\n",
    "                        np.round(Y[X[:,best_col] < self.split].mean()),\n",
    "                        np.round(Y[X[:,best_col] >= self.split].mean()),\n",
    "                    ]\n",
    "                    \"\"\"\n",
    "                    self.prediction contains the actual predictions \n",
    "                    can be called by self.prediction[0] for left and \n",
    "                    self.predictin[1] for right slips\n",
    "                    At the moment it has a 50% confidence interval i.e\n",
    "                    if there are more 1ns than 0s it will classify it as\n",
    "                    1 and vice versa. An additinal method can be implemented\n",
    "                    to increase the confidence interval to say 75%. This \n",
    "                    additional method will output one if its input is say\n",
    "                    over 0.75 and 0 if less.\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    # print \"best split:\", best_split\n",
    "                    left_idx = (X[:,best_col] < best_split)\n",
    "                    #print(left_idx)\n",
    "                    #left_idx - variable that holds true or false for each \n",
    "                    #sample in the best_col. \n",
    "                    Xleft = X[left_idx]\n",
    "                    #print(Xleft)\n",
    "                    Yleft = Y[left_idx]\n",
    "                    \n",
    "                    #Xleft and Yleft contain the data that was split\n",
    "                    #to the left of the condition. self.left then becomes\n",
    "                    #child node which is still of class TreeNode with \n",
    "                    #the depth incremented by 1\n",
    "                    \n",
    "                    self.left = DecisionTree(self.depth + 1, self.max_depth)\n",
    "                    self.left.fit(Xleft, Yleft)\n",
    "\n",
    "                    right_idx = (X[:,best_col] >= best_split)\n",
    "                    Xright = X[right_idx]\n",
    "                    Yright = Y[right_idx]\n",
    "                    self.right = DecisionTree(self.depth + 1, self.max_depth)\n",
    "                    self.right.fit(Xright, Yright)\n",
    "\n",
    "    def find_split(self, X, Y, col):\n",
    "        \"\"\"\n",
    "        Returns the information gain and the value at which the best split has\n",
    "        occured for that particular column. \n",
    "        The split is the middle point b/n 2 consequitive values. It is a \n",
    "        number.\n",
    "        Starts by arranging all samples in a column in an ascending order.\n",
    "        It then checks for the indexes in y where y changes from 0 to 1.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        x_values = X[:, col]\n",
    "        sort_idx = np.argsort(x_values)\n",
    "        x_values = x_values[sort_idx]\n",
    "        y_values = Y[sort_idx]\n",
    "\n",
    "        boundaries = np.nonzero(y_values[:-1] != y_values[1:])[0]\n",
    "        best_split = None\n",
    "        max_info_gain = 0\n",
    "        for b in boundaries:\n",
    "            split = (x_values[b] + x_values[b+1]) / 2\n",
    "            info_gain = self.information_gain(x_values, y_values, split)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_split = split\n",
    "        return max_info_gain, best_split\n",
    "\n",
    "    def information_gain(self, x, y, split):\n",
    "        \"\"\"\n",
    "        Computes the information gain on a given split value \n",
    "        \"\"\"\n",
    "        \n",
    "        y0 = y[x < split]\n",
    "        y1 = y[x >= split]\n",
    "        N = len(y)\n",
    "        y0len = len(y0)\n",
    "        if y0len == 0 or y0len == N:\n",
    "            return 0\n",
    "        p0 = float(len(y0)) / N\n",
    "        p1 = 1 - p0 \n",
    "        return entropy(y) - p0*entropy(y0) - p1*entropy(y1)\n",
    "\n",
    "    def single_sample_pred(self, x):\n",
    "        \"\"\"\n",
    "        Outputs the prediction on a single sample for a particular \n",
    "        attribute(column)\n",
    "        \"\"\"\n",
    "        # use \"is not None\" because 0 means False\n",
    "        if self.col is not None and self.split is not None:\n",
    "            feature = x[self.col]\n",
    "            if feature < self.split:\n",
    "                if self.left:\n",
    "                    p = self.left.single_sample_pred(x)\n",
    "                else:\n",
    "                    p = self.prediction[0]\n",
    "            else:\n",
    "                if self.right:\n",
    "                    p = self.right.single_sample_pred(x)\n",
    "                else:\n",
    "                    p = self.prediction[1]\n",
    "        else:\n",
    "            p = self.prediction\n",
    "        return p\n",
    "\n",
    "    def predict_all_samples(self, X):\n",
    "        \"\"\"\n",
    "        Calls predict_one on each sample. \n",
    "        \n",
    "        \"\"\"\n",
    "        N = len(X)\n",
    "        P = np.zeros(N)\n",
    "        for i in xrange(N):\n",
    "            P[i] = self.single_sample_pred(X[i])\n",
    "        return P\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        \"\"\"\n",
    "        Compares all the data points in the predicted class P to the \n",
    "        input class Y, and finds the mean. This then forms the accuracy of the \n",
    "        model.\n",
    "        \"\"\"\n",
    "        P = self.predict_all_samples(X)\n",
    "        return np.mean(P==Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.98546472863\n",
      "Test accuracy: 0.972933333333\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    X, Y = get_data_hr_churn()#uses the data from the churn project\n",
    "\n",
    "    \n",
    "\n",
    "    # split the data\n",
    "    Ntrain = len(Y) / 2\n",
    "    Xtrain, Ytrain = X[:Ntrain], Y[:Ntrain]\n",
    "    Xtest, Ytest = X[Ntrain:], Y[Ntrain:]\n",
    "    \n",
    "    model = DecisionTree(0,10)#Initialise a Decision Tree with a maximum depth of 10 \n",
    "    t0 = datetime.now()\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "\n",
    "    #Training Accuracy and time to compute training accuracy \n",
    "    print \"Train accuracy:\", model.score(Xtrain, Ytrain)\n",
    "\n",
    "    t0 = datetime.now()\n",
    "    #print(\"Prediction: \",model.predict(Xtest))\n",
    "    #Testing Accuracy and time to compute testing accuracy \n",
    "    print \"Test accuracy:\", model.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conslusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Decision Tree implementation without Sklearn yeilds training accuracy of 98.5% and testing accuracy of 97.3%. The values \n",
    "are overall very similar to those generated by the implementation using Sklearn provided above. \n",
    "In general increasing the depth of the decision tree results in higher accuracy as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
