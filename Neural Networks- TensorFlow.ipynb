{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition with a Multi-hidden Layer with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This project uses a Multi-hidden layer Neural Network for multiclass image classification. The neural network used is programmed using Tensorflow. The data is taken from Kaggle.The data consists of 48x48 pixel grayscale images of faces. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The relevant URL is:https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge The utils file contains all the necessary function that will be used such as activation functions(relu,tanh), indicator matrix function etc. In this project we use backpropagation to allow the neural network to be trained. In order the Neural Network to operate faster RMSPropOptimizer is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4953 1: 547 2: 5121 3: 8989 4: 6077 5: 4002 6: 6198 7: 0 8: 0 9: 35887\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from utils2 import getData, getBinaryData, indicator_mat_conv, error_rate_mean, init_weight_and_bias\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Hidden Layer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The hidden layer class initialises a hidden layer class with three attributes, input layer with M1 number of features, M2 number of output features and an id. It has one method, forward, which performs matrix multiplication and passes it to a tensorflow activation function,in this case relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, M1, M2, an_id):\n",
    "        self.id = an_id\n",
    "        self.M1 = M1\n",
    "        self.M2 = M2\n",
    "        W, b = init_weight_and_bias(M1, M2)\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.astype(np.float32))\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        return tf.nn.relu(tf.matmul(X, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is the main class in this project and it takes one attribute - hidden layer sizes.\n",
    "It contains a fit method contains most of functionality including: spitting the data into training and validataion(testing) sets, hidden layer initialisation, setting up the neural network cost by using softmax_cross_entropy_with_logits, setting up a tensorflow session and calling the RMSPropOptimiser to perform the backpropagation. This effectivelly substitutes all the derivatives equations from the project Neural Network for Binary Classification. It also defines two more methods - forward which performs matrix multiplication between the input data X and the weights and biasses which is used in cost calculation and prediction which uses argmax to obtain the index of the predicted class. The learning rate is set to 1e-2, the momentum has a value of 0.99, regularisation parameter has a value of 1e-3 and the batch size is of value 100. For convenience the batch size is set to 10 in order to speed up the calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNet(object):\n",
    "    def __init__(self, hidden_layer_sizes):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=1e-2, mu=0.99, decay=0.999, reg=1e-3, epochs=10, batch_sz=100, show_fig=False):\n",
    "        K = len(set(Y))# K contains all the classes that we are trying to predict.\n",
    "\n",
    "        # Spliting the data into training and validation sets\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X = X.astype(np.float32)\n",
    "        Y = indicator_mat_conv(Y).astype(np.float32)#performs one hot encoding on labels\n",
    "        # Y = Y.astype(np.int32)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]#1000 samples for validation(testing) the rest are reserved for learning\n",
    "        Yvalid_flat = np.argmax(Yvalid, axis=1) \n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "\n",
    "        # initialize hidden layers\n",
    "        N, D = X.shape\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        M1 = D\n",
    "        count = 0\n",
    "        #Initialise each hidden layer. The assignment statement M1=M2 is due to the fact that the output of each previous hidden\n",
    "        #layer becomes input to the next hidden layer\n",
    "        for M2 in self.hidden_layer_sizes:\n",
    "            h = HiddenLayer(M1, M2, count)\n",
    "            self.hidden_layers.append(h)\n",
    "            M1 = M2\n",
    "            count += 1\n",
    "        W, b = init_weight_and_bias(M1, K)\n",
    "        #Defining the W and b as tensorflow variables of type float32\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.astype(np.float32))\n",
    "\n",
    "        # The parameters stored in self.params will be used later\n",
    "        \n",
    "        self.params = [self.W, self.b]\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += h.params\n",
    "\n",
    "        # Setting up tensorflow placeholders for the data\n",
    "        # The act variable is passed as a parameter to softmax_cross_entropy_with_logits when the cost is \n",
    "        #calculated.\n",
    "        \n",
    "        tfX = tf.placeholder(tf.float32, shape=(None, D), name='X')\n",
    "        tfT = tf.placeholder(tf.float32, shape=(None, K), name='T')\n",
    "        act = self.forward(tfX)\n",
    "\n",
    "        rcost = reg*sum([tf.nn.l2_loss(p) for p in self.params])\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=act,\n",
    "                labels=tfT\n",
    "            )\n",
    "        ) + rcost\n",
    "        prediction = self.predict(tfX)\n",
    "        # Using RMSPropOptimizer to minimise the cost. It effectivelly performs gradient descent with some optimisations to \n",
    "        #make it calculate faster.\n",
    "        \n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate, decay=decay, momentum=mu).minimize(cost)\n",
    "\n",
    "        n_batches = N // batch_sz#setting up the batches \n",
    "        costs = []\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        #starting up a session\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            for i in range(epochs):\n",
    "                X, Y = shuffle(X, Y)\n",
    "                for j in range(n_batches):\n",
    "                    Xbatch = X[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "                    Ybatch = Y[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "\n",
    "                    session.run(train_op, feed_dict={tfX: Xbatch, tfT: Ybatch})\n",
    "\n",
    "                    if j % 20 == 0:\n",
    "                        #Will be printing each 20th sample of each batch\n",
    "                        c = session.run(cost, feed_dict={tfX: Xvalid, tfT: Yvalid})\n",
    "                        costs.append(c)\n",
    "\n",
    "                        p = session.run(prediction, feed_dict={tfX: Xvalid, tfT: Yvalid})\n",
    "                        e = error_rate_mean(Yvalid_flat, p)\n",
    "                        print(\"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c, \"error rate:\", e)\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "            \n",
    "    #Defining the forward and predict methods\n",
    "    def forward(self, X):\n",
    "        Z = X\n",
    "        for h in self.hidden_layers:\n",
    "            Z = h.forward(Z)\n",
    "        return tf.matmul(Z, self.W) + self.b\n",
    "    # Performs the actual prediction i.e classifies the output.\n",
    "    def predict(self, X):\n",
    "        act = self.forward(X)\n",
    "        return tf.argmax(act, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here the main class will be initialised. The neural network is initialised with three hidden layers as stated above with sizes of 2000,1000 and 500 units respectivelly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 j: 0 nb: 348 cost: 3.69076 error rate: 0.837\n",
      "i: 0 j: 20 nb: 348 cost: 3.57621 error rate: 0.817\n",
      "i: 0 j: 40 nb: 348 cost: 3.49857 error rate: 0.724\n",
      "i: 0 j: 60 nb: 348 cost: 3.47054 error rate: 0.724\n",
      "i: 0 j: 80 nb: 348 cost: 3.43008 error rate: 0.724\n",
      "i: 0 j: 100 nb: 348 cost: 3.37122 error rate: 0.708\n",
      "i: 0 j: 120 nb: 348 cost: 3.34308 error rate: 0.684\n",
      "i: 0 j: 140 nb: 348 cost: 3.24996 error rate: 0.67\n",
      "i: 0 j: 160 nb: 348 cost: 3.25777 error rate: 0.716\n",
      "i: 0 j: 180 nb: 348 cost: 3.16058 error rate: 0.678\n",
      "i: 0 j: 200 nb: 348 cost: 3.07922 error rate: 0.671\n",
      "i: 0 j: 220 nb: 348 cost: 3.05901 error rate: 0.669\n",
      "i: 0 j: 240 nb: 348 cost: 3.02296 error rate: 0.688\n",
      "i: 0 j: 260 nb: 348 cost: 2.99217 error rate: 0.701\n",
      "i: 0 j: 280 nb: 348 cost: 2.92335 error rate: 0.675\n",
      "i: 0 j: 300 nb: 348 cost: 2.86585 error rate: 0.702\n",
      "i: 0 j: 320 nb: 348 cost: 2.79299 error rate: 0.673\n",
      "i: 0 j: 340 nb: 348 cost: 2.8155 error rate: 0.717\n",
      "i: 1 j: 0 nb: 348 cost: 2.73725 error rate: 0.674\n",
      "i: 1 j: 20 nb: 348 cost: 2.70342 error rate: 0.68\n",
      "i: 1 j: 40 nb: 348 cost: 2.704 error rate: 0.685\n",
      "i: 1 j: 60 nb: 348 cost: 2.61149 error rate: 0.665\n",
      "i: 1 j: 80 nb: 348 cost: 2.56314 error rate: 0.672\n",
      "i: 1 j: 100 nb: 348 cost: 2.51735 error rate: 0.661\n",
      "i: 1 j: 120 nb: 348 cost: 2.54598 error rate: 0.747\n",
      "i: 1 j: 140 nb: 348 cost: 2.4557 error rate: 0.677\n",
      "i: 1 j: 160 nb: 348 cost: 2.46258 error rate: 0.71\n",
      "i: 1 j: 180 nb: 348 cost: 2.44623 error rate: 0.691\n",
      "i: 1 j: 200 nb: 348 cost: 2.38442 error rate: 0.684\n",
      "i: 1 j: 220 nb: 348 cost: 2.37183 error rate: 0.687\n",
      "i: 1 j: 240 nb: 348 cost: 2.30058 error rate: 0.659\n",
      "i: 1 j: 260 nb: 348 cost: 2.31049 error rate: 0.704\n",
      "i: 1 j: 280 nb: 348 cost: 2.26459 error rate: 0.68\n",
      "i: 1 j: 300 nb: 348 cost: 2.23899 error rate: 0.679\n",
      "i: 1 j: 320 nb: 348 cost: 2.19086 error rate: 0.656\n",
      "i: 1 j: 340 nb: 348 cost: 2.18421 error rate: 0.677\n",
      "i: 2 j: 0 nb: 348 cost: 2.16025 error rate: 0.667\n",
      "i: 2 j: 20 nb: 348 cost: 2.20303 error rate: 0.716\n",
      "i: 2 j: 40 nb: 348 cost: 2.15926 error rate: 0.705\n",
      "i: 2 j: 60 nb: 348 cost: 2.16488 error rate: 0.702\n",
      "i: 2 j: 80 nb: 348 cost: 2.08157 error rate: 0.687\n",
      "i: 2 j: 100 nb: 348 cost: 2.05751 error rate: 0.682\n",
      "i: 2 j: 120 nb: 348 cost: 2.03729 error rate: 0.687\n",
      "i: 2 j: 140 nb: 348 cost: 2.05377 error rate: 0.694\n",
      "i: 2 j: 160 nb: 348 cost: 2.02138 error rate: 0.669\n",
      "i: 2 j: 180 nb: 348 cost: 2.04277 error rate: 0.719\n",
      "i: 2 j: 200 nb: 348 cost: 1.97149 error rate: 0.673\n",
      "i: 2 j: 220 nb: 348 cost: 1.95917 error rate: 0.672\n",
      "i: 2 j: 240 nb: 348 cost: 1.98402 error rate: 0.675\n",
      "i: 2 j: 260 nb: 348 cost: 1.95528 error rate: 0.682\n",
      "i: 2 j: 280 nb: 348 cost: 1.94278 error rate: 0.701\n",
      "i: 2 j: 300 nb: 348 cost: 1.9182 error rate: 0.676\n",
      "i: 2 j: 320 nb: 348 cost: 1.91276 error rate: 0.671\n",
      "i: 2 j: 340 nb: 348 cost: 1.89644 error rate: 0.666\n",
      "i: 3 j: 0 nb: 348 cost: 1.88379 error rate: 0.657\n",
      "i: 3 j: 20 nb: 348 cost: 1.89103 error rate: 0.678\n",
      "i: 3 j: 40 nb: 348 cost: 1.90382 error rate: 0.691\n",
      "i: 3 j: 60 nb: 348 cost: 1.92865 error rate: 0.712\n",
      "i: 3 j: 80 nb: 348 cost: 1.87571 error rate: 0.69\n",
      "i: 3 j: 100 nb: 348 cost: 1.87262 error rate: 0.667\n",
      "i: 3 j: 120 nb: 348 cost: 1.87776 error rate: 0.675\n",
      "i: 3 j: 140 nb: 348 cost: 1.84263 error rate: 0.683\n",
      "i: 3 j: 160 nb: 348 cost: 1.84198 error rate: 0.677\n",
      "i: 3 j: 180 nb: 348 cost: 1.83013 error rate: 0.676\n",
      "i: 3 j: 200 nb: 348 cost: 1.84665 error rate: 0.674\n",
      "i: 3 j: 220 nb: 348 cost: 1.83262 error rate: 0.689\n",
      "i: 3 j: 240 nb: 348 cost: 1.82019 error rate: 0.676\n",
      "i: 3 j: 260 nb: 348 cost: 1.80134 error rate: 0.673\n",
      "i: 3 j: 280 nb: 348 cost: 1.79201 error rate: 0.668\n",
      "i: 3 j: 300 nb: 348 cost: 1.77819 error rate: 0.669\n",
      "i: 3 j: 320 nb: 348 cost: 1.87085 error rate: 0.682\n",
      "i: 3 j: 340 nb: 348 cost: 1.88361 error rate: 0.71\n",
      "i: 4 j: 0 nb: 348 cost: 1.88883 error rate: 0.745\n",
      "i: 4 j: 20 nb: 348 cost: 1.848 error rate: 0.716\n",
      "i: 4 j: 40 nb: 348 cost: 1.84263 error rate: 0.721\n",
      "i: 4 j: 60 nb: 348 cost: 1.81197 error rate: 0.702\n",
      "i: 4 j: 80 nb: 348 cost: 1.82108 error rate: 0.703\n",
      "i: 4 j: 100 nb: 348 cost: 1.80076 error rate: 0.683\n",
      "i: 4 j: 120 nb: 348 cost: 1.82808 error rate: 0.72\n",
      "i: 4 j: 140 nb: 348 cost: 1.84749 error rate: 0.679\n",
      "i: 4 j: 160 nb: 348 cost: 1.81398 error rate: 0.69\n",
      "i: 4 j: 180 nb: 348 cost: 1.77452 error rate: 0.678\n",
      "i: 4 j: 200 nb: 348 cost: 1.79221 error rate: 0.686\n",
      "i: 4 j: 220 nb: 348 cost: 1.80619 error rate: 0.701\n",
      "i: 4 j: 240 nb: 348 cost: 1.88322 error rate: 0.72\n",
      "i: 4 j: 260 nb: 348 cost: 1.82811 error rate: 0.712\n",
      "i: 4 j: 280 nb: 348 cost: 1.82285 error rate: 0.711\n",
      "i: 4 j: 300 nb: 348 cost: 1.8117 error rate: 0.704\n",
      "i: 4 j: 320 nb: 348 cost: 1.78312 error rate: 0.689\n",
      "i: 4 j: 340 nb: 348 cost: 1.79997 error rate: 0.692\n",
      "i: 5 j: 0 nb: 348 cost: 1.77743 error rate: 0.686\n",
      "i: 5 j: 20 nb: 348 cost: 1.80211 error rate: 0.702\n",
      "i: 5 j: 40 nb: 348 cost: 1.83637 error rate: 0.746\n",
      "i: 5 j: 60 nb: 348 cost: 1.85444 error rate: 0.765\n",
      "i: 5 j: 80 nb: 348 cost: 1.78761 error rate: 0.7\n",
      "i: 5 j: 100 nb: 348 cost: 1.8227 error rate: 0.709\n",
      "i: 5 j: 120 nb: 348 cost: 1.82203 error rate: 0.719\n",
      "i: 5 j: 140 nb: 348 cost: 1.83669 error rate: 0.688\n",
      "i: 5 j: 160 nb: 348 cost: 1.84036 error rate: 0.723\n",
      "i: 5 j: 180 nb: 348 cost: 1.83497 error rate: 0.723\n",
      "i: 5 j: 200 nb: 348 cost: 1.82849 error rate: 0.723\n",
      "i: 5 j: 220 nb: 348 cost: 1.8277 error rate: 0.723\n",
      "i: 5 j: 240 nb: 348 cost: 1.83219 error rate: 0.723\n",
      "i: 5 j: 260 nb: 348 cost: 1.82582 error rate: 0.723\n",
      "i: 5 j: 280 nb: 348 cost: 1.82025 error rate: 0.723\n",
      "i: 5 j: 300 nb: 348 cost: 1.81707 error rate: 0.723\n",
      "i: 5 j: 320 nb: 348 cost: 1.81822 error rate: 0.723\n",
      "i: 5 j: 340 nb: 348 cost: 1.81133 error rate: 0.723\n",
      "i: 6 j: 0 nb: 348 cost: 1.81179 error rate: 0.723\n",
      "i: 6 j: 20 nb: 348 cost: 1.81086 error rate: 0.723\n",
      "i: 6 j: 40 nb: 348 cost: 1.8091 error rate: 0.723\n",
      "i: 6 j: 60 nb: 348 cost: 1.81273 error rate: 0.723\n",
      "i: 6 j: 80 nb: 348 cost: 1.80189 error rate: 0.723\n",
      "i: 6 j: 100 nb: 348 cost: 1.80502 error rate: 0.723\n",
      "i: 6 j: 120 nb: 348 cost: 1.8 error rate: 0.723\n",
      "i: 6 j: 140 nb: 348 cost: 1.80717 error rate: 0.723\n",
      "i: 6 j: 160 nb: 348 cost: 1.80067 error rate: 0.723\n",
      "i: 6 j: 180 nb: 348 cost: 1.79856 error rate: 0.723\n",
      "i: 6 j: 200 nb: 348 cost: 1.80036 error rate: 0.723\n",
      "i: 6 j: 220 nb: 348 cost: 1.80055 error rate: 0.723\n",
      "i: 6 j: 240 nb: 348 cost: 1.79731 error rate: 0.723\n",
      "i: 6 j: 260 nb: 348 cost: 1.79422 error rate: 0.723\n",
      "i: 6 j: 280 nb: 348 cost: 1.79927 error rate: 0.723\n",
      "i: 6 j: 300 nb: 348 cost: 1.79702 error rate: 0.723\n",
      "i: 6 j: 320 nb: 348 cost: 1.79702 error rate: 0.723\n",
      "i: 6 j: 340 nb: 348 cost: 1.79406 error rate: 0.723\n",
      "i: 7 j: 0 nb: 348 cost: 1.79389 error rate: 0.723\n",
      "i: 7 j: 20 nb: 348 cost: 1.79665 error rate: 0.723\n",
      "i: 7 j: 40 nb: 348 cost: 1.7944 error rate: 0.723\n",
      "i: 7 j: 60 nb: 348 cost: 1.79669 error rate: 0.723\n",
      "i: 7 j: 80 nb: 348 cost: 1.7961 error rate: 0.723\n",
      "i: 7 j: 100 nb: 348 cost: 1.79824 error rate: 0.723\n",
      "i: 7 j: 120 nb: 348 cost: 1.79981 error rate: 0.723\n",
      "i: 7 j: 140 nb: 348 cost: 1.79355 error rate: 0.723\n",
      "i: 7 j: 160 nb: 348 cost: 1.80998 error rate: 0.723\n",
      "i: 7 j: 180 nb: 348 cost: 1.79572 error rate: 0.723\n",
      "i: 7 j: 200 nb: 348 cost: 1.79897 error rate: 0.723\n",
      "i: 7 j: 220 nb: 348 cost: 1.78842 error rate: 0.723\n",
      "i: 7 j: 240 nb: 348 cost: 1.80818 error rate: 0.723\n",
      "i: 7 j: 260 nb: 348 cost: 1.79659 error rate: 0.723\n",
      "i: 7 j: 280 nb: 348 cost: 1.79668 error rate: 0.723\n",
      "i: 7 j: 300 nb: 348 cost: 1.7915 error rate: 0.723\n",
      "i: 7 j: 320 nb: 348 cost: 1.79853 error rate: 0.723\n",
      "i: 7 j: 340 nb: 348 cost: 1.79296 error rate: 0.723\n",
      "i: 8 j: 0 nb: 348 cost: 1.79062 error rate: 0.723\n",
      "i: 8 j: 20 nb: 348 cost: 1.79305 error rate: 0.723\n",
      "i: 8 j: 40 nb: 348 cost: 1.79687 error rate: 0.723\n",
      "i: 8 j: 60 nb: 348 cost: 1.79883 error rate: 0.723\n",
      "i: 8 j: 80 nb: 348 cost: 1.79648 error rate: 0.723\n",
      "i: 8 j: 100 nb: 348 cost: 1.79814 error rate: 0.723\n",
      "i: 8 j: 120 nb: 348 cost: 1.80018 error rate: 0.723\n",
      "i: 8 j: 140 nb: 348 cost: 1.80422 error rate: 0.723\n",
      "i: 8 j: 160 nb: 348 cost: 1.79373 error rate: 0.723\n",
      "i: 8 j: 180 nb: 348 cost: 1.79471 error rate: 0.723\n",
      "i: 8 j: 200 nb: 348 cost: 1.78937 error rate: 0.723\n",
      "i: 8 j: 220 nb: 348 cost: 1.79764 error rate: 0.723\n",
      "i: 8 j: 240 nb: 348 cost: 1.80237 error rate: 0.723\n",
      "i: 8 j: 260 nb: 348 cost: 1.7899 error rate: 0.723\n",
      "i: 8 j: 280 nb: 348 cost: 1.79216 error rate: 0.723\n",
      "i: 8 j: 300 nb: 348 cost: 1.79785 error rate: 0.723\n",
      "i: 8 j: 320 nb: 348 cost: 1.79223 error rate: 0.723\n",
      "i: 8 j: 340 nb: 348 cost: 1.79733 error rate: 0.723\n",
      "i: 9 j: 0 nb: 348 cost: 1.79464 error rate: 0.723\n",
      "i: 9 j: 20 nb: 348 cost: 1.80878 error rate: 0.831\n",
      "i: 9 j: 40 nb: 348 cost: 1.7893 error rate: 0.723\n",
      "i: 9 j: 60 nb: 348 cost: 1.80409 error rate: 0.723\n",
      "i: 9 j: 80 nb: 348 cost: 1.81064 error rate: 0.723\n",
      "i: 9 j: 100 nb: 348 cost: 1.79725 error rate: 0.723\n",
      "i: 9 j: 120 nb: 348 cost: 1.79117 error rate: 0.723\n",
      "i: 9 j: 140 nb: 348 cost: 1.81419 error rate: 0.723\n",
      "i: 9 j: 160 nb: 348 cost: 1.79891 error rate: 0.723\n",
      "i: 9 j: 180 nb: 348 cost: 1.79933 error rate: 0.723\n",
      "i: 9 j: 200 nb: 348 cost: 1.80819 error rate: 0.723\n",
      "i: 9 j: 220 nb: 348 cost: 1.80422 error rate: 0.723\n",
      "i: 9 j: 240 nb: 348 cost: 1.79973 error rate: 0.723\n",
      "i: 9 j: 260 nb: 348 cost: 1.81529 error rate: 0.831\n",
      "i: 9 j: 280 nb: 348 cost: 1.7988 error rate: 0.723\n",
      "i: 9 j: 300 nb: 348 cost: 1.80743 error rate: 0.723\n",
      "i: 9 j: 320 nb: 348 cost: 1.80897 error rate: 0.723\n",
      "i: 9 j: 340 nb: 348 cost: 1.79737 error rate: 0.723\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxpggY8FvjDFBxoLfGGOCzP8HiehYgevTjbwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d24e5e26d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    X, Y = getData()\n",
    "    # X, Y = getBinaryData()\n",
    "    model = NNet([2000, 1000, 500])\n",
    "    model.fit(X, Y, show_fig=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
